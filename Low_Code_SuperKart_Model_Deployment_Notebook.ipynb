{"cells":[{"cell_type":"markdown","id":"k-xVqOB6QTm6","metadata":{"id":"k-xVqOB6QTm6"},"source":["# **Problem Statement**"]},{"cell_type":"markdown","id":"AJDbTeirQasg","metadata":{"id":"AJDbTeirQasg"},"source":["## Business Context"]},{"cell_type":"markdown","id":"94bMSnpWQmuA","metadata":{"id":"94bMSnpWQmuA"},"source":["A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.\n","\n","Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future."]},{"cell_type":"markdown","id":"Aasy7LC_Qpq5","metadata":{"id":"Aasy7LC_Qpq5"},"source":["## Objective"]},{"cell_type":"markdown","id":"khshBslaQtX9","metadata":{"id":"khshBslaQtX9"},"source":["SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.\n","\n","To operationalize these insights at scale, the company has partnered with a data science firm—not just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKart’s decision-making systems and used across its network of stores."]},{"cell_type":"markdown","id":"v-HxlIhTQ0-E","metadata":{"id":"v-HxlIhTQ0-E"},"source":["## Data Description"]},{"cell_type":"markdown","id":"c0670116","metadata":{"id":"c0670116"},"source":["The data contains the different attributes of the various products and stores.The detailed data dictionary is given below.\n","\n","- **Product_Id** - unique identifier of each product, each identifier having two letters at the beginning followed by a number.\n","- **Product_Weight** - weight of each product\n","- **Product_Sugar_Content** - sugar content of each product like low sugar, regular and no sugar\n","- **Product_Allocated_Area** - ratio of the allocated display area of each product to the total display area of all the products in a store\n","- **Product_Type** - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\n","- **Product_MRP** - maximum retail price of each product\n","- **Store_Id** - unique identifier of each store\n","- **Store_Establishment_Year** - year in which the store was established\n","- **Store_Size** - size of the store depending on sq. feet like high, medium and low\n","- **Store_Location_City_Type** - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.\n","- **Store_Type** - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart\n","- **Product_Store_Sales_Total** - total revenue generated by the sale of that particular product in that particular store\n"]},{"cell_type":"markdown","source":["# **Please read the instructions carefully before starting the project.**"],"metadata":{"id":"EWGlpDNkrTqA"},"id":"EWGlpDNkrTqA"},{"cell_type":"markdown","source":["This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n","* Blanks '_____' are provided in the notebook that\n","needs to be filled with an appropriate code to get the correct result. With every '_____' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n","* Identify the task to be performed correctly, and only then proceed to write the required code.\n","* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n","* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same. Any mathematical or computational details which are a graded part of the project can be included in the Appendix section of the presentation."],"metadata":{"id":"oIAAXUOip1Fc"},"id":"oIAAXUOip1Fc"},{"cell_type":"markdown","id":"60VhOlydQ-PG","metadata":{"id":"60VhOlydQ-PG"},"source":["# **Installing and Importing the necessary libraries**"]},{"cell_type":"code","execution_count":null,"id":"yisDjKOB6TqF","metadata":{"id":"yisDjKOB6TqF"},"outputs":[],"source":["#Installing the libraries with the specified versions\n","!pip install numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 joblib==1.4.2 xgboost==2.1.4 requests==2.32.3 huggingface_hub==0.30.1 -q"]},{"cell_type":"markdown","id":"m-wZg1XZ6bLa","metadata":{"id":"m-wZg1XZ6bLa"},"source":["**Note:**\n","\n","- After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\n","\n","- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."]},{"cell_type":"code","execution_count":null,"id":"c0022e4d","metadata":{"id":"c0022e4d"},"outputs":[],"source":["# Libraries to help with reading and manipulating data\n","import numpy as np\n","import pandas as pd\n","\n","# For splitting the dataset\n","from sklearn.model_selection import train_test_split\n","\n","# Libaries to help with data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Removes the limit for the number of displayed columns\n","pd.set_option(\"display.max_columns\", None)\n","# Sets the limit for the number of displayed rows\n","pd.set_option(\"display.max_rows\", 100)\n","\n","\n","# Libraries different ensemble classifiers\n","from sklearn.ensemble import (\n","    BaggingRegressor,\n","    RandomForestRegressor,\n","    AdaBoostRegressor,\n","    GradientBoostingRegressor,\n",")\n","from xgboost import XGBRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# Libraries to get different metric scores\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    accuracy_score,\n","    precision_score,\n","    recall_score,\n","    f1_score,\n","    mean_squared_error,\n","    mean_absolute_error,\n","    r2_score,\n","    mean_absolute_percentage_error\n",")\n","\n","# To create the pipeline\n","from sklearn.compose import make_column_transformer\n","from sklearn.pipeline import make_pipeline,Pipeline\n","\n","# To tune different models and standardize\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler,OneHotEncoder\n","\n","# To serialize the model\n","import joblib\n","\n","# os related functionalities\n","import os\n","\n","# API request\n","import requests\n","\n","# for hugging face space authentication to upload files\n","from huggingface_hub import login, HfApi\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","id":"51b91836","metadata":{"id":"51b91836"},"source":["# **Loading the dataset**"]},{"cell_type":"code","execution_count":null,"id":"tW5rHBxTRUEj","metadata":{"id":"tW5rHBxTRUEj"},"outputs":[],"source":["# Uncomment the below snippet of code if the drive needs to be mounted\n","#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"db0d776a","metadata":{"id":"db0d776a"},"outputs":[],"source":["kart = pd.read_csv(\"_____\") #Complete the code to read the data"]},{"cell_type":"code","execution_count":null,"id":"3a41759a","metadata":{"id":"3a41759a"},"outputs":[],"source":["# copying data to another variable to avoid any changes to original data\n","data = kart.copy()"]},{"cell_type":"markdown","id":"W2sXwrUERYua","metadata":{"id":"W2sXwrUERYua"},"source":["# **Data Overview**"]},{"cell_type":"markdown","id":"65c8c14d","metadata":{"id":"65c8c14d"},"source":["## View the first and last 5 rows of the dataset"]},{"cell_type":"code","execution_count":null,"id":"27d5a6d3","metadata":{"id":"27d5a6d3"},"outputs":[],"source":["data._____ #Complete the code to display the first 5 rows of the dataset"]},{"cell_type":"code","execution_count":null,"id":"cb5ec8ca","metadata":{"id":"cb5ec8ca"},"outputs":[],"source":["data._____ #Complete the code to display the last 5 rows of the dataset"]},{"cell_type":"markdown","id":"ee04adcd","metadata":{"id":"ee04adcd"},"source":["## Understand the shape of the dataset"]},{"cell_type":"code","execution_count":null,"id":"e9040b24","metadata":{"id":"e9040b24"},"outputs":[],"source":["print(f\"There are {data.shape[0]} rows and {data.shape[_____]} columns.\")  #Complete the code to display the number of columns"]},{"cell_type":"markdown","id":"e25abfe4","metadata":{"id":"e25abfe4"},"source":["## Check the data types of the columns for the dataset"]},{"cell_type":"code","execution_count":null,"id":"0e8cc237","metadata":{"id":"0e8cc237","scrolled":true},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","id":"9e76c12a","metadata":{"id":"9e76c12a"},"source":["## Statistical summary of the data"]},{"cell_type":"markdown","id":"f4ab23ef","metadata":{"id":"f4ab23ef"},"source":["**Let's check the statistical summary of the data.**"]},{"cell_type":"code","execution_count":null,"id":"ae93b01c","metadata":{"id":"ae93b01c"},"outputs":[],"source":["data.describe(include=\"all\").T"]},{"cell_type":"markdown","id":"J3auOiwfUXWf","metadata":{"id":"J3auOiwfUXWf"},"source":["## Checking for duplicate values"]},{"cell_type":"code","execution_count":null,"id":"489451d2","metadata":{"id":"489451d2"},"outputs":[],"source":["# checking for duplicate values\n","data.duplicated().sum()"]},{"cell_type":"markdown","id":"w0SIhAByUoHh","metadata":{"id":"w0SIhAByUoHh"},"source":["## Checking for missing values"]},{"cell_type":"code","execution_count":null,"id":"cf1d6de3","metadata":{"id":"cf1d6de3"},"outputs":[],"source":["# checking for missing values in the data\n","data.isnull()._____ #Complete the code to compute the number of missing values."]},{"cell_type":"markdown","id":"4oamAwxrVHLq","metadata":{"id":"4oamAwxrVHLq"},"source":["# **Exploratory Data Analysis (EDA)**"]},{"cell_type":"markdown","id":"21341d2b","metadata":{"id":"21341d2b"},"source":["## Univariate Analysis"]},{"cell_type":"code","execution_count":null,"id":"c1ebcb3d","metadata":{"id":"c1ebcb3d"},"outputs":[],"source":["# function to plot a boxplot and a histogram along the same scale.\n","\n","def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n","    \"\"\"\n","    Boxplot and histogram combined\n","\n","    data: dataframe\n","    feature: dataframe column\n","    figsize: size of figure (default (12,7))\n","    kde: whether to the show density curve (default False)\n","    bins: number of bins for histogram (default None)\n","    \"\"\"\n","    f2, (ax_box2, ax_hist2) = plt.subplots(\n","        nrows=2,  # Number of rows of the subplot grid= 2\n","        sharex=True,  # x-axis will be shared among all subplots\n","        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n","        figsize=figsize,\n","    )  # creating the 2 subplots\n","    sns.boxplot(\n","        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n","    )  # boxplot will be created and a star will indicate the mean value of the column\n","    sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n","    ) if bins else sns.histplot(\n","        data=data, x=feature, kde=kde, ax=ax_hist2\n","    )  # For histogram\n","    ax_hist2.axvline(\n","        data[feature].mean(), color=\"green\", linestyle=\"--\"\n","    )  # Add mean to the histogram\n","    ax_hist2.axvline(\n","        data[feature].median(), color=\"black\", linestyle=\"-\"\n","    )  # Add median to the histogram"]},{"cell_type":"markdown","id":"afe06238","metadata":{"id":"afe06238"},"source":["### **Product_Weight**"]},{"cell_type":"code","execution_count":null,"id":"dbc43098","metadata":{"id":"dbc43098"},"outputs":[],"source":["histogram_boxplot(data, \"Product_Weight\")"]},{"cell_type":"markdown","id":"a23bdd73","metadata":{"id":"a23bdd73"},"source":["### **Product_Allocated_Area**"]},{"cell_type":"code","execution_count":null,"id":"71792a43","metadata":{"id":"71792a43"},"outputs":[],"source":["histogram_boxplot(data, \"_____\") #Complete the code to plot the boxplot and histogram of Product_Allocated_Area"]},{"cell_type":"markdown","id":"a3242fb5","metadata":{"id":"a3242fb5"},"source":["### **Product_MRP**"]},{"cell_type":"code","execution_count":null,"id":"4c401dde","metadata":{"id":"4c401dde"},"outputs":[],"source":["histogram_boxplot(data, \"_____\") #Complete the code to plot the boxplot and histogram of Product_MRP"]},{"cell_type":"markdown","id":"2a53e9ac","metadata":{"id":"2a53e9ac"},"source":["### **Product_Store_Sales_Total**"]},{"cell_type":"code","execution_count":null,"id":"1cee157d","metadata":{"id":"1cee157d"},"outputs":[],"source":["histogram_boxplot(data, \"_____\") #Complete the code to plot the boxplot and histogram of Product_Store_Sales_Total"]},{"cell_type":"code","execution_count":null,"id":"1a1f7bdd","metadata":{"id":"1a1f7bdd"},"outputs":[],"source":["# function to create labeled barplots\n","\n","\n","def labeled_barplot(data, feature, perc=False, n=None):\n","    \"\"\"\n","    Barplot with percentage at the top\n","\n","    data: dataframe\n","    feature: dataframe column\n","    perc: whether to display percentages instead of count (default is False)\n","    n: displays the top n category levels (default is None, i.e., display all levels)\n","    \"\"\"\n","\n","    total = len(data[feature])  # length of the column\n","    count = data[feature].nunique()\n","    if n is None:\n","        plt.figure(figsize=(count + 1, 5))\n","    else:\n","        plt.figure(figsize=(n + 1, 5))\n","\n","    plt.xticks(rotation=90, fontsize=15)\n","    ax = sns.countplot(\n","        data=data,\n","        x=feature,\n","        palette=\"Paired\",\n","        order=data[feature].value_counts().index[:n].sort_values(),\n","    )\n","\n","    for p in ax.patches:\n","        if perc == True:\n","            label = \"{:.1f}%\".format(\n","                100 * p.get_height() / total\n","            )  # percentage of each class of the category\n","        else:\n","            label = p.get_height()  # count of each level of the category\n","\n","        x = p.get_x() + p.get_width() / 2  # width of the plot\n","        y = p.get_height()  # height of the plot\n","\n","        ax.annotate(\n","            label,\n","            (x, y),\n","            ha=\"center\",\n","            va=\"center\",\n","            size=12,\n","            xytext=(0, 5),\n","            textcoords=\"offset points\",\n","        )  # annotate the percentage\n","\n","    plt.show()  # show the plot"]},{"cell_type":"markdown","id":"af2e3e97","metadata":{"id":"af2e3e97"},"source":["### **Product_Sugar_Content**"]},{"cell_type":"code","execution_count":null,"id":"1f4128d2","metadata":{"id":"1f4128d2"},"outputs":[],"source":["labeled_barplot(data, \"Product_Sugar_Content\", perc=True)"]},{"cell_type":"markdown","id":"65a65843","metadata":{"id":"65a65843"},"source":["### **Product_Type**"]},{"cell_type":"code","execution_count":null,"id":"bd5c9fc6","metadata":{"id":"bd5c9fc6"},"outputs":[],"source":["labeled_barplot(data, \"_____\", perc=_____) #Complete the code to plot the labelled barplot of Product_Type with the percentages being displayed"]},{"cell_type":"markdown","id":"da06e6d9","metadata":{"id":"da06e6d9"},"source":["### **Store_Id**"]},{"cell_type":"code","execution_count":null,"id":"6bb68360","metadata":{"id":"6bb68360"},"outputs":[],"source":["labeled_barplot(data, \"_____\", perc=_____) #Complete the code to plot the labelled barplot of Store_Id with the percentages being displayed"]},{"cell_type":"markdown","id":"6a20880d","metadata":{"id":"6a20880d"},"source":["### **Store_Size**"]},{"cell_type":"code","execution_count":null,"id":"d335d873","metadata":{"id":"d335d873"},"outputs":[],"source":["labeled_barplot(data, \"_____\", perc=_____) #Complete the code to plot the labelled barplot of Store_Size with the percentages being displayed"]},{"cell_type":"markdown","id":"971f7446","metadata":{"id":"971f7446"},"source":["### **Store_Location_City_Type**"]},{"cell_type":"code","execution_count":null,"id":"59dacd5d","metadata":{"id":"59dacd5d"},"outputs":[],"source":["labeled_barplot(data, \"_____\", perc=_____) #Complete the code to plot the labelled barplot of Store_Location_City_Type with the percentages being displayed"]},{"cell_type":"markdown","id":"9475b863","metadata":{"id":"9475b863"},"source":["### **Store_Type**"]},{"cell_type":"code","execution_count":null,"id":"e53299bb","metadata":{"id":"e53299bb"},"outputs":[],"source":["labeled_barplot(data, \"_____\", perc=_____) #Complete the code to plot the labelled barplot of Store_Type with the percentages being displayed"]},{"cell_type":"markdown","id":"46f0b9af","metadata":{"id":"46f0b9af"},"source":["## Bivariate Analysis"]},{"cell_type":"markdown","id":"05mWTy8wMUKG","metadata":{"id":"05mWTy8wMUKG"},"source":["### **Correlation matrix**"]},{"cell_type":"code","execution_count":null,"id":"896fd22b","metadata":{"id":"896fd22b"},"outputs":[],"source":["cols_list = data.select_dtypes(include=np.number).columns.tolist()\n","\n","plt.figure(figsize=(10, 5))\n","sns.heatmap(\n","    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",")\n","plt.show()"]},{"cell_type":"markdown","id":"02b68b47","metadata":{"id":"02b68b47"},"source":["### **Let's check the distribution of our target variable i.e Product_Store_Sales_Total with the numeric columns**"]},{"cell_type":"code","execution_count":null,"id":"12daa38c","metadata":{"id":"12daa38c"},"outputs":[],"source":["plt.figure(figsize=[8, 6])\n","sns.scatterplot(x=data.Product_Weight, y=data.Product_Store_Sales_Total)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"06153b97","metadata":{"id":"06153b97"},"outputs":[],"source":["plt.figure(figsize=[8, 6])\n","sns.scatterplot(x=_____, y=_____) #Complete the code to plot a scatterplot of Product_Allocated_Area and Product_Store_Sales_Total\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"3aa62580","metadata":{"id":"3aa62580"},"outputs":[],"source":["plt.figure(figsize=[8, 6])\n","sns.scatterplot(x=_____, y=_____) #Complete the code to plot a scatterplot of Product_MRP and Product_Store_Sales_Total\n","plt.show()"]},{"cell_type":"markdown","id":"9292a578","metadata":{"id":"9292a578"},"source":["### **Let us see from which product type the company is generating most of the revenue**"]},{"cell_type":"code","execution_count":null,"id":"ca6cdaaa","metadata":{"id":"ca6cdaaa"},"outputs":[],"source":["df_revenue1 = data.groupby([\"Product_Type\"], as_index=False)[\n","    \"Product_Store_Sales_Total\"\n","].sum()\n","plt.figure(figsize=[14, 8])\n","plt.xticks(rotation=90)\n","a = sns.barplot(x=df_revenue1.Product_Type, y=df_revenue1.Product_Store_Sales_Total)\n","a.set_xlabel(\"Product Types\")\n","a.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1c9f4b5f","metadata":{"id":"1c9f4b5f"},"outputs":[],"source":["df_revenue2 = data.groupby([\"_____\"], as_index=False)[\n","    \"_____\"\n","].sum()  #Complete the code to perform a groupby on Product_Sugar_Content and select Product_Store_Sales_Total\n","plt.figure(figsize=[8, 6])\n","plt.xticks(rotation=90)\n","b = sns.barplot(\n","    x=df_revenue2.Product_Sugar_Content, y=df_revenue2.Product_Store_Sales_Total\n",")\n","b.set_xlabel(\"Product_Sugar_content\")\n","b.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"markdown","id":"7301694d","metadata":{"id":"7301694d"},"source":["### **Let us see from which type of stores and locations the revenue generation is more**."]},{"cell_type":"code","execution_count":null,"id":"090b043f","metadata":{"id":"090b043f"},"outputs":[],"source":["df_store_revenue = data.groupby([\"_____\"], as_index=False)[\n","    \"_____\"\n","].sum() #Complete the code to perform a groupby on Store_Id and select Product_Store_Sales_Total\n","plt.figure(figsize=[8, 6])\n","plt.xticks(rotation=90)\n","r = sns.barplot(\n","    x=df_store_revenue.Store_Id, y=df_store_revenue.Product_Store_Sales_Total\n",")\n","r.set_xlabel(\"Stores\")\n","r.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c9b5b592","metadata":{"id":"c9b5b592"},"outputs":[],"source":["df_revenue3 = data.groupby([\"_____\"], as_index=False)[\n","    \"_____\"\n","].sum() #Complete the code to perform a groupby on Store_Size and select Product_Store_Sales_Total\n","plt.figure(figsize=[8, 6])\n","plt.xticks(rotation=90)\n","c = sns.barplot(x=df_revenue3.Store_Size, y=df_revenue3.Product_Store_Sales_Total)\n","c.set_xlabel(\"Store_Size\")\n","c.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"3c3f0288","metadata":{"id":"3c3f0288"},"outputs":[],"source":["df_revenue4 = data.groupby([\"_____\"], as_index=False)[\n","    \"_____\"\n","].sum() #Complete the code to perform a groupby on Store_Location_City_Type and select Product_Store_Sales_Total\n","plt.figure(figsize=[8, 6])\n","plt.xticks(rotation=90)\n","d = sns.barplot(\n","    x=df_revenue4.Store_Location_City_Type, y=df_revenue4.Product_Store_Sales_Total\n",")\n","d.set_xlabel(\"Store_Location_City_Type\")\n","d.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"e55ec4e1","metadata":{"id":"e55ec4e1"},"outputs":[],"source":["df_revenue5 = data.groupby([\"_____\"], as_index=False)[\n","    \"_____\"\n","].sum() #Complete the code to perform a groupby on Store_Type and select Product_Store_Sales_Total\n","plt.figure(figsize=[8, 6])\n","plt.xticks(rotation=90)\n","e = sns.barplot(x=df_revenue5.Store_Type, y=df_revenue5.Product_Store_Sales_Total)\n","e.set_xlabel(\"Store_Type\")\n","e.set_ylabel(\"Revenue\")\n","plt.show()"]},{"cell_type":"markdown","id":"e8ecab60","metadata":{"id":"e8ecab60"},"source":["### **Let's check the distribution of our target variable i.e Product_Store_Sales_Total with the other categorical columns**"]},{"cell_type":"code","execution_count":null,"id":"70fccb54","metadata":{"id":"70fccb54"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data=data, x=\"Store_Id\", y=\"Product_Store_Sales_Total\", hue = \"Store_Id\")\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Store_Id Vs Product_Store_Sales_Total\")\n","plt.xlabel(\"Stores\")\n","plt.ylabel(\"Product_Store_Sales_Total (of each product)\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"806d977c","metadata":{"id":"806d977c"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data = data, x = \"_____\", y = \"_____\", hue = \"_____\") #Complet the code to plot the boxplot with x as Store_Size , y as Product_Store_Sales_Total and hue as Store_Size\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Store_Size Vs Product_Store_Sales_Total\")\n","plt.xlabel(\"Stores\")\n","plt.ylabel(\"Product_Store_Sales_Total (of each product)\")\n","plt.show()"]},{"cell_type":"markdown","id":"02523ef7","metadata":{"id":"02523ef7"},"source":["### **Let's now try to find out some relationship between the other columns**"]},{"cell_type":"code","execution_count":null,"id":"f79b3e0d","metadata":{"id":"f79b3e0d"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data = data, x = \"_____\", y = \"_____\", hue = \"_____\") #Complete the code to plot the boxplot with x as Product_Type , y as Product_Weight and hue as Product_Type\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Product_Type Vs Product_Weight\")\n","plt.xlabel(\"Types of Products\")\n","plt.ylabel(\"Product_Weight\")\n","plt.show()"]},{"cell_type":"markdown","id":"9d6b86b0","metadata":{"id":"9d6b86b0"},"source":["### **Let's find out whether there is some relationship between the weight of the product and its sugar content**"]},{"cell_type":"code","execution_count":null,"id":"34df8ae3","metadata":{"id":"34df8ae3"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data = data, x = \"_____\", y = \"_____\", hue = \"_____\") #Complete the code to plot the boxplot with x as Product_Sugar_Content , y as Product_Weight and hue as Product_Sugar_Content\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Product_Sugar_Content Vs Product_Weight\")\n","plt.xlabel(\"Product_Sugar_Content\")\n","plt.ylabel(\"Product_Weight\")\n","plt.show()"]},{"cell_type":"markdown","id":"c2bd287f","metadata":{"id":"c2bd287f"},"source":["### **Let's analyze the sugar content of different product types**"]},{"cell_type":"code","execution_count":null,"id":"e349075a","metadata":{"id":"e349075a"},"outputs":[],"source":["plt.figure(figsize=(14, 8))\n","sns.heatmap(\n","    pd.crosstab(data[\"Product_Sugar_Content\"], data[\"Product_Type\"]),\n","    annot=True,\n","    fmt=\"g\",\n","    cmap=\"viridis\",\n",")\n","plt.ylabel(\"Product_Sugar_Content\")\n","plt.xlabel(\"Product_Type\")\n","plt.show()"]},{"cell_type":"markdown","id":"88533e8f","metadata":{"id":"88533e8f"},"source":["### **Let's find out how many items of each product type has been sold in each of the stores**"]},{"cell_type":"code","execution_count":null,"id":"c04a0c1e","metadata":{"id":"c04a0c1e"},"outputs":[],"source":["plt.figure(figsize=(14, 8))\n","sns.heatmap(\n","    pd.crosstab(data[\"_____\"], data[\"_____\"]), #Complete the code to perform a crosstab operation between Store_Id and Product_Type\n","    annot=True,\n","    fmt=\"g\",\n","    cmap=\"viridis\",\n",")\n","plt.ylabel(\"Stores\")\n","plt.xlabel(\"Product_Type\")\n","plt.show()"]},{"cell_type":"markdown","id":"f47c5e18","metadata":{"id":"f47c5e18"},"source":["### **Different product types have different prices. Let's analyze the trend.**"]},{"cell_type":"code","execution_count":null,"id":"8f538ae2","metadata":{"id":"8f538ae2"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data = data, x = \"_____\", y = \"_____\", hue = \"_____\") #Complete the code to plot a boxplot with x as Product_Type , y as Product_MRP and hue as Product_Type\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Product_Type Vs Product_MRP\")\n","plt.xlabel(\"Product_Type\")\n","plt.ylabel(\"Product_MRP (of each product)\")\n","plt.show()"]},{"cell_type":"markdown","id":"d1bb6c77","metadata":{"id":"d1bb6c77"},"source":["### **Let's find out how the Product_MRP varies with the different stores**"]},{"cell_type":"code","execution_count":null,"id":"cb496e1d","metadata":{"id":"cb496e1d"},"outputs":[],"source":["plt.figure(figsize=[14, 8])\n","sns.boxplot(data = data, x = \"_____\", y = \"_____\", hue = \"_____\") #Complete the code to plot the boxplot with x as Store_Id , y as Product_MRP and hue as Store_Id\n","plt.xticks(rotation=90)\n","plt.title(\"Boxplot - Store_Id Vs Product_MRP\")\n","plt.xlabel(\"Stores\")\n","plt.ylabel(\"Product_MRP (of each product)\")\n","plt.show()"]},{"cell_type":"markdown","id":"09d79669","metadata":{"id":"09d79669"},"source":["### **Let's delve deeper and do a detailed analysis of each of the stores**."]},{"cell_type":"markdown","id":"4e457801","metadata":{"id":"4e457801"},"source":["#### OUT001"]},{"cell_type":"code","execution_count":null,"id":"d67d5a44","metadata":{"id":"d67d5a44"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT001\"].describe(include=\"all\").T"]},{"cell_type":"markdown","id":"45de3d68","metadata":{"id":"45de3d68"},"source":["**Observations**\n","- OUT001 is a store of Supermarket Type 1 which is located in a Tier 2 city and has store size as high. It was established in 1987.\n","- OUT001 has sold products whose MRP range from 71 to 227.\n","- Snack Foods have been sold the highest number of times in OUT001.\n","- The revenue generated from each product at OUT001 ranges from 2300 to 5000."]},{"cell_type":"code","execution_count":null,"id":"e57ead84","metadata":{"id":"e57ead84"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT001\", \"Product_Store_Sales_Total\"].sum()"]},{"cell_type":"markdown","id":"c0fa1b47","metadata":{"id":"c0fa1b47"},"source":["**OUT001 has generated total revenue of 6223113 from the sales of goods.**"]},{"cell_type":"code","execution_count":null,"id":"cd9ac2da","metadata":{"id":"cd9ac2da"},"outputs":[],"source":["df_OUT001 = (\n","    data.loc[data[\"Store_Id\"] == \"OUT001\"]\n","    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n","    .sum()\n",")\n","plt.figure(figsize=[14, 8])\n","plt.xticks(rotation=90)\n","plt.xlabel(\"Product_Type\")\n","plt.ylabel(\"Product_Store_Sales_Total\")\n","plt.title(\"OUT001\")\n","sns.barplot(x=df_OUT001.Product_Type, y=df_OUT001.Product_Store_Sales_Total)\n","plt.show()"]},{"cell_type":"markdown","id":"c7b1f9f1","metadata":{"id":"c7b1f9f1"},"source":["- OUT001 has generated the highest revenue from the sale of fruits and vegetables and snack foods. Both the categories have contributed around 800000 each."]},{"cell_type":"markdown","id":"937e0981","metadata":{"id":"937e0981"},"source":["#### OUT002"]},{"cell_type":"code","execution_count":null,"id":"6537c822","metadata":{"id":"6537c822"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT002\"].describe(include=\"all\").T"]},{"cell_type":"markdown","id":"e5d31efe","metadata":{"id":"e5d31efe"},"source":["**Observations**\n","- OUT002 is a food mart which is located in a Tier 3 city and has store size as small. It was established in 1998.\n","- OUT002 has sold products whose MRP range from 31 to 225.\n","- Fruits and vegetables have been sold the highest number of times in OUT002.\n","- The revenue generated from each product at OUT002 ranges from 33 to 2300."]},{"cell_type":"code","execution_count":null,"id":"d5abba53","metadata":{"id":"d5abba53"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT002\", \"Product_Store_Sales_Total\"].sum()"]},{"cell_type":"markdown","id":"89191414","metadata":{"id":"89191414"},"source":["**OUT002 has generated total revenue of 2030910 from the sales of goods.**"]},{"cell_type":"code","execution_count":null,"id":"dd2a2253","metadata":{"id":"dd2a2253"},"outputs":[],"source":["df_OUT002 = (\n","    data.loc[data[\"Store_Id\"] == \"OUT002\"]\n","    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n","    .sum()\n",")\n","plt.figure(figsize=[14, 8])\n","plt.xticks(rotation=90)\n","plt.xlabel(\"Product_Type\")\n","plt.ylabel(\"Product_Store_Sales_Total\")\n","plt.title(\"OUT002\")\n","sns.barplot(x=df_OUT002.Product_Type, y=df_OUT002.Product_Store_Sales_Total)\n","plt.show()"]},{"cell_type":"markdown","id":"8b81a5c1","metadata":{"id":"8b81a5c1"},"source":["- OUT002 has generated the highest revenue from the sale of fruits and vegetables (~ 300000) followed by snack foods (~ 250000)."]},{"cell_type":"markdown","id":"ae5b4f72","metadata":{"id":"ae5b4f72"},"source":["#### OUT003"]},{"cell_type":"code","execution_count":null,"id":"5f3e70f8","metadata":{"id":"5f3e70f8"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT003\"].describe(include=\"all\").T"]},{"cell_type":"markdown","id":"35035878","metadata":{"id":"35035878"},"source":["**Observations**\n","- OUT003 is a Departmental store which is located in a Tier 1 city and has store size as medium. It was established in 1999.\n","- OUT003 has sold products whose MRP range from 86 to 266.\n","- Snack Foods have been sold the highest number of times in OUT003.\n","- The revenue generated from each product at OUT003 ranges from 3070 to 8000."]},{"cell_type":"code","execution_count":null,"id":"0f647c1a","metadata":{"id":"0f647c1a"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT003\", \"Product_Store_Sales_Total\"].sum()"]},{"cell_type":"markdown","id":"0483a070","metadata":{"id":"0483a070"},"source":["**OUT003 has generated total revenue of 6673458 from the sales of goods.**"]},{"cell_type":"code","execution_count":null,"id":"54073d44","metadata":{"id":"54073d44"},"outputs":[],"source":["df_OUT003 = (\n","    data.loc[data[\"Store_Id\"] == \"OUT003\"]\n","    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n","    .sum()\n",")\n","plt.figure(figsize=[14, 8])\n","plt.xticks(rotation=90)\n","plt.xlabel(\"Product_Type\")\n","plt.ylabel(\"Product_Store_Sales_Total\")\n","plt.title(\"OUT003\")\n","sns.barplot(x=df_OUT003.Product_Type, y=df_OUT003.Product_Store_Sales_Total)\n","plt.show()"]},{"cell_type":"markdown","id":"09f1fb5c","metadata":{"id":"09f1fb5c"},"source":["- OUT003 has generated the highest revenue from the sale of snack foods followed by fruits and vegetables, both the categories contributing around 800000 each."]},{"cell_type":"markdown","id":"b5ecd021","metadata":{"id":"b5ecd021"},"source":["#### OUT004"]},{"cell_type":"code","execution_count":null,"id":"54aa971f","metadata":{"id":"54aa971f"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT004\"].describe(include=\"all\").T"]},{"cell_type":"markdown","id":"ed24d5a7","metadata":{"id":"ed24d5a7"},"source":["**Observations**\n","- OUT004 is a store of Supermarket Type2 which is located in a Tier 2 city and has store size as medium. It was established in 2009.\n","- OUT004 has sold products whose MRP range from 83 to 198.\n","- Fruits and vegetables have been sold the highest number of times in OUT004.\n","- The revenue generated from each product at OUT004 ranges from 1561 to 5463."]},{"cell_type":"code","execution_count":null,"id":"b1a60f82","metadata":{"id":"b1a60f82"},"outputs":[],"source":["data.loc[data[\"Store_Id\"] == \"OUT004\", \"Product_Store_Sales_Total\"].sum()"]},{"cell_type":"markdown","id":"fc03a722","metadata":{"id":"fc03a722"},"source":["**OUT004 has generated total revenue of 15427583 from the sales of goods which is highest among all the 4 stores.**"]},{"cell_type":"code","execution_count":null,"id":"f6d1305f","metadata":{"id":"f6d1305f"},"outputs":[],"source":["df_OUT004 = (\n","    data.loc[data[\"Store_Id\"] == \"OUT004\"]\n","    .groupby([\"Product_Type\"], as_index=False)[\"Product_Store_Sales_Total\"]\n","    .sum()\n",")\n","plt.figure(figsize=[14, 8])\n","plt.xticks(rotation=90)\n","plt.xlabel(\"Product_Type\")\n","plt.ylabel(\"Product_Store_Sales_Total\")\n","plt.title(\"OUT004\")\n","sns.barplot(x=df_OUT004.Product_Type, y=df_OUT004.Product_Store_Sales_Total)\n","plt.show()"]},{"cell_type":"markdown","id":"33e4f198","metadata":{"id":"33e4f198"},"source":["- OUT004 has generated the highest revenue from the sale of fruits and vegetables (~ 2500000) followed by snack foods (~ 2000000)."]},{"cell_type":"markdown","id":"9ef8562c","metadata":{"id":"9ef8562c"},"source":["**Let's find out the revenue generated by the stores from each of the product types**."]},{"cell_type":"code","execution_count":null,"id":"f7337f4f","metadata":{"id":"f7337f4f"},"outputs":[],"source":["df1 = data.groupby([\"Product_Type\", \"Store_Id\"], as_index=False)[\n","    \"Product_Store_Sales_Total\"\n","].sum()\n","df1"]},{"cell_type":"markdown","id":"fccffb14","metadata":{"id":"fccffb14"},"source":["- In all the product types, the revenue generated by OUT004 has been the highest which seems quite logical since around 53% of the total products were brought from this store.\n","- In all the product categories, the revenue generated by OUT002 has been the lowest which seems quite obvious since it is small store in a Tier 3 city."]},{"cell_type":"markdown","id":"f3083f23","metadata":{"id":"f3083f23"},"source":["**Let's find out the revenue generated by the stores from products having different levels of sugar content**."]},{"cell_type":"code","execution_count":null,"id":"0167eedf","metadata":{"id":"0167eedf"},"outputs":[],"source":["df2 = data.groupby([\"Product_Sugar_Content\", \"Store_Id\"], as_index=False)[\n","    \"Product_Store_Sales_Total\"\n","].sum()\n","df2"]},{"cell_type":"markdown","id":"ba3192c9","metadata":{"id":"ba3192c9"},"source":["- The trend is the same as that which was present in the revenue analysis of stores for product types."]},{"cell_type":"markdown","id":"0fo5OvIfVdtB","metadata":{"id":"0fo5OvIfVdtB"},"source":["# **Data Preprocessing**"]},{"cell_type":"markdown","id":"12651542","metadata":{"id":"12651542"},"source":["## **Replacing the values in the Product_Sugar_Content column**"]},{"cell_type":"markdown","id":"0d861520","metadata":{"id":"0d861520"},"source":["We can observe that in the Product_Sugar_Content column, there are 3 types - Low Sugar, Regular and reg.\n","\n","It seems quite obvious that Regular and reg are referring to the same category. So let's replace reg with Regular."]},{"cell_type":"code","execution_count":null,"id":"f22d9e30","metadata":{"id":"f22d9e30"},"outputs":[],"source":["# Replacing reg with Regular\n","data.Product_Sugar_Content.replace(to_replace=[\"reg\"], value=[\"Regular\"], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"9468d673","metadata":{"id":"9468d673"},"outputs":[],"source":["data.Product_Sugar_Content.value_counts()"]},{"cell_type":"markdown","id":"2bf4362a","metadata":{"id":"2bf4362a"},"source":["## **Exploring Patterns in Product_IDs**"]},{"cell_type":"markdown","id":"ltFNpsQv7vEM","metadata":{"id":"ltFNpsQv7vEM"},"source":["We can see that the Product_Id column has two characters followed by a number.\n","\n","Let's delve deeper and see whether they are having any relationship with the other columns or not"]},{"cell_type":"code","execution_count":null,"id":"43bf839c","metadata":{"id":"43bf839c"},"outputs":[],"source":["## extracting the first two characters from the Product_Id column and storing it in another column\n","data[\"Product_Id_char\"] = data[\"Product_Id\"].str[:2]\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"88ea2400","metadata":{"id":"88ea2400"},"outputs":[],"source":["data[\"Product_Id_char\"].unique()"]},{"cell_type":"code","execution_count":null,"id":"4af41bb0","metadata":{"id":"4af41bb0"},"outputs":[],"source":["data.loc[data.Product_Id_char == \"FD\", \"Product_Type\"].unique()"]},{"cell_type":"code","execution_count":null,"id":"745dd04a","metadata":{"id":"745dd04a"},"outputs":[],"source":["data.loc[data.Product_Id_char == \"_____+\", \"Product_Type\"].unique() #Complete the code to select the rows where Product_Id_char is DR"]},{"cell_type":"code","execution_count":null,"id":"55aac458","metadata":{"id":"55aac458"},"outputs":[],"source":["data.loc[data.Product_Id_char == \"_____\", \"Product_Type\"].unique() #Complete the code to select the rows where Product_Id_char is NC"]},{"cell_type":"markdown","id":"Ey6xQv8Eal1x","metadata":{"id":"Ey6xQv8Eal1x"},"source":["## **Store's Age**"]},{"cell_type":"markdown","id":"Bn0RWmL97PMy","metadata":{"id":"Bn0RWmL97PMy"},"source":["A store which has been in the business for a long duration is more trustworthy than the newly established ones.\n","\n","On the other hand, older stores may sometimes lack infrastructure if proper attention is not given. So let us calculate the current age of the store and incorporate that in our model."]},{"cell_type":"code","execution_count":null,"id":"c18c2219","metadata":{"id":"c18c2219"},"outputs":[],"source":["# Outlet Age\n","data[\"Store_Age_Years\"] = 2025 - data.Store_Establishment_Year"]},{"cell_type":"markdown","id":"27b7992e","metadata":{"id":"27b7992e"},"source":["## **Grouping Product Types into Perishables and Non-Perishables.**"]},{"cell_type":"markdown","id":"6qjo1Mvw7khE","metadata":{"id":"6qjo1Mvw7khE"},"source":["We have 16 different product types in our dataset.\n","\n","So let us make two broad categories, perishables and non perishables, in order to reduce the number of product types"]},{"cell_type":"code","execution_count":null,"id":"f67aec38","metadata":{"id":"f67aec38"},"outputs":[],"source":["perishables = [\n","    \"Dairy\",\n","    \"Meat\",\n","    \"Fruits and Vegetables\",\n","    \"Breakfast\",\n","    \"Breads\",\n","    \"Seafood\",\n","]"]},{"cell_type":"code","execution_count":null,"id":"bdf7bc3a","metadata":{"id":"bdf7bc3a"},"outputs":[],"source":["def change(x):\n","    if x in perishables:\n","        return \"Perishables\"\n","    else:\n","        return \"Non Perishables\""]},{"cell_type":"code","execution_count":null,"id":"O9Na_k-pFNtR","metadata":{"id":"O9Na_k-pFNtR"},"outputs":[],"source":["data['Product_Type_Category'] = data['Product_Type'].apply(change)"]},{"cell_type":"code","execution_count":null,"id":"e1e1c5ca","metadata":{"id":"e1e1c5ca"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","id":"fef26ce9","metadata":{"id":"fef26ce9"},"source":["## **Outlier Check**"]},{"cell_type":"code","execution_count":null,"id":"5e04f7e5","metadata":{"id":"5e04f7e5"},"outputs":[],"source":["# outlier detection using boxplot\n","numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n","numeric_columns.remove(\"Store_Establishment_Year\")\n","numeric_columns.remove(\"Store_Age_Years\")\n","\n","\n","plt.figure(figsize=(15, 12))\n","\n","for i, variable in enumerate(numeric_columns):\n","    plt.subplot(4, 4, i + 1)\n","    plt.boxplot(data[variable], whis=1.5)\n","    plt.tight_layout()\n","    plt.title(variable)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"-8pT_8miXe8I","metadata":{"id":"-8pT_8miXe8I"},"source":["## **Data Preparation for Modeling**"]},{"cell_type":"markdown","id":"70970767","metadata":{"id":"70970767"},"source":["- We aim to forecast the Product_Store_Sales_Total.\n","\n","- Before building the model, we'll drop unnecessary columns and encode the categorical features.\n","\n","- We'll then split the data into training and testing sets to evaluate the model's performance on unseen data."]},{"cell_type":"code","execution_count":null,"id":"5b990d8b","metadata":{"id":"5b990d8b"},"outputs":[],"source":["data.head()"]},{"cell_type":"markdown","id":"bpNp6QnTe5N2","metadata":{"id":"bpNp6QnTe5N2"},"source":["Let's remove the columns that are not required."]},{"cell_type":"code","execution_count":null,"id":"6dc255ba","metadata":{"id":"6dc255ba"},"outputs":[],"source":["data = data.drop([\"_____\"], axis=1) #Complete the code to drop the columns \"Product_Id\",\"Product_Type\",\"Store_Id\",\"Store_Establishment_Year\""]},{"cell_type":"code","execution_count":null,"id":"b429f181","metadata":{"id":"b429f181"},"outputs":[],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"id":"62ff524c","metadata":{"id":"62ff524c","scrolled":true},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"id":"dc13395b","metadata":{"id":"dc13395b"},"outputs":[],"source":["# Separating features and the target column\n","X = data.drop(\"_____\", axis=1) #Complete the code to drop the target variable\n","y = data[\"_____\"] #Complete the code to select the target variable"]},{"cell_type":"code","execution_count":null,"id":"61736fd2","metadata":{"id":"61736fd2"},"outputs":[],"source":["# Splitting the data into train and test sets in 70:30 ratio\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=_____, random_state=1, shuffle=True #Complete the code to define the test_size\n",")"]},{"cell_type":"code","execution_count":null,"id":"IU70X3UzECcF","metadata":{"id":"IU70X3UzECcF"},"outputs":[],"source":["X_train.shape, X_test.shape"]},{"cell_type":"markdown","id":"ORwoDUPvkqEt","metadata":{"id":"ORwoDUPvkqEt"},"source":["### **Data Pre-processing Pipeline**"]},{"cell_type":"code","execution_count":null,"id":"kIxa57hVmNub","metadata":{"id":"kIxa57hVmNub"},"outputs":[],"source":["categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()\n","categorical_features"]},{"cell_type":"code","execution_count":null,"id":"RfPYiSS-D4KV","metadata":{"id":"RfPYiSS-D4KV"},"outputs":[],"source":["# Create a preprocessing pipeline for the categorical features\n","\n","preprocessor = make_column_transformer(\n","    (Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))]), categorical_features)\n",")"]},{"cell_type":"markdown","id":"5fd3cabe","metadata":{"id":"5fd3cabe"},"source":["# **Model Building**"]},{"cell_type":"markdown","source":["<h2><b>Note: As per the rubric, you are required to build two ML models for this project. We have provided the code required for model building for six different ML models, each under a separate markdown section.  You may choose any two models from the ones provided below, uncomment the corresponding code for the model, and then run the code to build the model, and check its performance.</b></h2>"],"metadata":{"id":"AA51JSZgyeeo"},"id":"AA51JSZgyeeo"},{"cell_type":"markdown","id":"YyzOQ8pBY93N","metadata":{"id":"YyzOQ8pBY93N"},"source":["## Define functions for Model Evaluation"]},{"cell_type":"markdown","id":"48cd8dd0","metadata":{"id":"48cd8dd0"},"source":["- We'll fit different models on the train data and observe their performance.\n","- We'll try to improve that performance by tuning some hyperparameters available for that algorithm.\n","- We'll use GridSearchCv for hyperparameter tuning and `r_2 score` to optimize the model.\n","- R-square - `Coefficient of determination` is used to evaluate the performance of a regression model. It is the amount of the variation in the output dependent attribute which is predictable from the input independent variables.\n","- Let's start by creating a function to get model scores, so that we don't have to use the same codes repeatedly."]},{"cell_type":"code","execution_count":null,"id":"d107c3d3","metadata":{"id":"d107c3d3"},"outputs":[],"source":["# function to compute adjusted R-squared\n","def adj_r2_score(predictors, targets, predictions):\n","    r2 = r2_score(targets, predictions)\n","    n = predictors.shape[0]\n","    k = predictors.shape[1]\n","    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n","\n","\n","# function to compute different metrics to check performance of a regression model\n","def model_performance_regression(model, predictors, target):\n","    \"\"\"\n","    Function to compute different metrics to check regression model performance\n","\n","    model: regressor\n","    predictors: independent variables\n","    target: dependent variable\n","    \"\"\"\n","\n","    # predicting using the independent variables\n","    pred = model.predict(predictors)\n","\n","    r2 = r2_score(target, pred)  # to compute R-squared\n","    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n","    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n","    mae = mean_absolute_error(target, pred)  # to compute MAE\n","    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE\n","\n","    # creating a dataframe of metrics\n","    df_perf = pd.DataFrame(\n","        {\n","            \"RMSE\": rmse,\n","            \"MAE\": mae,\n","            \"R-squared\": r2,\n","            \"Adj. R-squared\": adjr2,\n","            \"MAPE\": mape,\n","        },\n","        index=[0],\n","    )\n","\n","    return df_perf"]},{"cell_type":"markdown","id":"c498d89d","metadata":{"id":"c498d89d"},"source":["## Decision Tree Model"]},{"cell_type":"code","execution_count":null,"id":"d1a8d0fa","metadata":{"id":"d1a8d0fa"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","# dtree = DecisionTreeRegressor(random_state=1)\n","# dtree = make_pipeline(preprocessor,dtree)\n","# dtree.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"96109b81","metadata":{"id":"96109b81"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"44b9d822","metadata":{"id":"44b9d822"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","#dtree_model_train_perf = model_performance_regression(dtree, X_train, y_train)\n","#dtree_model_train_perf"]},{"cell_type":"markdown","id":"7bc83ccd","metadata":{"id":"7bc83ccd"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"6d7aa19f","metadata":{"id":"6d7aa19f"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","#dtree_model_test_perf = model_performance_regression(dtree, X_test, y_test)\n","#dtree_model_test_perf"]},{"cell_type":"markdown","id":"f5be37da","metadata":{"id":"f5be37da"},"source":["## Bagging Regressor"]},{"cell_type":"code","execution_count":null,"id":"b7db6067","metadata":{"id":"b7db6067"},"outputs":[],"source":["# Uncomment the below snippet of code if baggingregressor is to be used\n","\n","#bagging_regressor = BaggingRegressor(random_state=1)\n","#bagging_regressor = make_pipeline(preprocessor,bagging_regressor)\n","#bagging_regressor.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"a795d478","metadata":{"id":"a795d478"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"4f84d865","metadata":{"id":"4f84d865"},"outputs":[],"source":["# Uncomment the below snippet of code if baggingregressor is to be used\n","\n","#bagging_regressor_model_train_perf = model_performance_regression(bagging_regressor, X_train, y_train)\n","#bagging_regressor_model_train_perf"]},{"cell_type":"markdown","id":"7d6b38b8","metadata":{"id":"7d6b38b8"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"44a393b6","metadata":{"id":"44a393b6"},"outputs":[],"source":["# Uncomment the below snippet of code if baggingregressor is to be used\n","\n","#bagging_regressor_model_test_perf = model_performance_regression(bagging_regressor, X_test, y_test)\n","#bagging_regressor_model_test_perf"]},{"cell_type":"markdown","id":"118e3496","metadata":{"id":"118e3496"},"source":["## Random Forest Model"]},{"cell_type":"code","execution_count":null,"id":"6329988b","metadata":{"id":"6329988b"},"outputs":[],"source":["# Uncomment the below snippet of code if random forest regressor is to be used\n","\n","#rf_estimator = RandomForestRegressor(random_state=1)\n","#rf_estimator = make_pipeline(preprocessor,rf_estimator)\n","#rf_estimator.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"6671af9a","metadata":{"id":"6671af9a"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"e6eeecc8","metadata":{"id":"e6eeecc8"},"outputs":[],"source":["# Uncomment the below snippet of code if random forest regressor is to be used\n","\n","#rf_estimator_model_train_perf = model_performance_regression(rf_estimator, X_train, y_train)\n","#rf_estimator_model_train_perf"]},{"cell_type":"markdown","id":"26479ea1","metadata":{"id":"26479ea1"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"8740b2a8","metadata":{"id":"8740b2a8"},"outputs":[],"source":["# Uncomment the below snippet of code if random forest regressor is to be used\n","\n","#rf_estimator_model_test_perf = model_performance_regression(rf_estimator, X_test, y_test)\n","#rf_estimator_model_test_perf"]},{"cell_type":"markdown","id":"e4980b89","metadata":{"id":"e4980b89"},"source":["## AdaBoost Regressor"]},{"cell_type":"code","execution_count":null,"id":"965ce7bb","metadata":{"id":"965ce7bb"},"outputs":[],"source":["# Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","#ab_regressor = AdaBoostRegressor(random_state=1)\n","#ab_regressor = make_pipeline(preprocessor,ab_regressor)\n","#ab_regressor.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"7863e4bc","metadata":{"id":"7863e4bc"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"b953d4b8","metadata":{"id":"b953d4b8"},"outputs":[],"source":["# Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","#ab_regressor_model_train_perf = model_performance_regression(ab_regressor, X_train, y_train)\n","#ab_regressor_model_train_perf"]},{"cell_type":"markdown","id":"991a69b1","metadata":{"id":"991a69b1"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"9f07917e","metadata":{"id":"9f07917e"},"outputs":[],"source":["# Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","#ab_regressor_model_test_perf = model_performance_regression(ab_regressor, X_test, y_test)\n","#ab_regressor_model_test_perf"]},{"cell_type":"markdown","id":"206f5d57","metadata":{"id":"206f5d57"},"source":["## Gradient Boosting Regressor"]},{"cell_type":"code","execution_count":null,"id":"6a87071a","metadata":{"id":"6a87071a"},"outputs":[],"source":["# Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","#gb_estimator = GradientBoostingRegressor(random_state=1)\n","#gb_estimator = make_pipeline(preprocessor,gb_estimator)\n","#gb_estimator.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"7aac56e7","metadata":{"id":"7aac56e7"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"7d6f26c5","metadata":{"id":"7d6f26c5"},"outputs":[],"source":["# Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","#gb_estimator_model_train_perf = model_performance_regression(gb_estimator, X_train, y_train)\n","#gb_estimator_model_train_perf"]},{"cell_type":"markdown","id":"aa0fb9ba","metadata":{"id":"aa0fb9ba"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"790e0176","metadata":{"id":"790e0176"},"outputs":[],"source":["# Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","#gb_estimator_model_test_perf = model_performance_regression(gb_estimator, X_test, y_test)\n","#gb_estimator_model_test_perf"]},{"cell_type":"markdown","id":"ffbda60a","metadata":{"id":"ffbda60a"},"source":["## XGBoost Regressor"]},{"cell_type":"code","execution_count":null,"id":"a1ad20ba","metadata":{"id":"a1ad20ba"},"outputs":[],"source":["# Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","#xgb_estimator = XGBRegressor(random_state=1)\n","#xgb_estimator = make_pipeline(preprocessor,xgb_estimator)\n","#xgb_estimator.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"67b31c4c","metadata":{"id":"67b31c4c"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"da72e1fd","metadata":{"id":"da72e1fd"},"outputs":[],"source":["# Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","#xgb_estimator_model_train_perf = model_performance_regression(xgb_estimator, X_train, y_train)\n","#xgb_estimator_model_train_perf"]},{"cell_type":"markdown","id":"dd20f197","metadata":{"id":"dd20f197"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"622c0ae2","metadata":{"id":"622c0ae2"},"outputs":[],"source":["# Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","#xgb_estimator_model_test_perf = model_performance_regression(xgb_estimator, X_test, y_test)\n","#xgb_estimator_model_test_perf"]},{"cell_type":"markdown","id":"FtkIDTjdYy5h","metadata":{"id":"FtkIDTjdYy5h"},"source":["# **Model Performance Improvement - Hyperparameter Tuning**"]},{"cell_type":"markdown","source":["<h2><b>Note:</b></h2>\n","\n","<h2><b>1. As per the rubric, you are required to tune two ML models for this project. We have provided the code required for model building for six different ML models, each under a separate markdown section.  You may choose the two models you built previously, uncomment the corresponding code for the model, and then run the code to tune the model, and check its performance.</b></h2>\n","<h2><b>2. We've provided a sample parameter grid for tuning. You may add/remove parameters or parameter values to check for during tuning as per your requirements.</b></h2>"],"metadata":{"id":"qpftDJS9zAQg"},"id":"qpftDJS9zAQg"},{"cell_type":"markdown","id":"37341729","metadata":{"id":"37341729"},"source":["## Hyperparameter Tuning - Decision Tree"]},{"cell_type":"code","execution_count":null,"id":"7781bc35","metadata":{"id":"7781bc35"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","# # Choose the type of classifier.\n","# dtree_tuned = DecisionTreeRegressor(random_state=1)\n","# dtree_tuned = make_pipeline(preprocessor,dtree_tuned)\n","\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"decisiontreeregressor__max_depth\": list(np.arange(2, 6)),\n","#     \"decisiontreeregressor__min_samples_leaf\": [1, 3, 5],\n","#     \"decisiontreeregressor__max_leaf_nodes\": [2, 3, 5, 10, 15],\n","#     \"decisiontreeregressor__min_impurity_decrease\": [0.001, 0.01, 0.1],\n","# }\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=r2_score, cv=3, n_jobs =-1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# dtree_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# dtree_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"2c88356c","metadata":{"id":"2c88356c"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"cccc3f91","metadata":{"id":"cccc3f91"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","#dtree_tuned_model_train_perf = model_performance_regression(dtree_tuned, X_train, y_train)\n","#dtree_tuned_model_train_perf"]},{"cell_type":"markdown","id":"58048bb7","metadata":{"id":"58048bb7"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"a9aabc27","metadata":{"id":"a9aabc27"},"outputs":[],"source":["# Uncomment the below snippet of code if decision tree regressor is to be used\n","\n","#dtree_tuned_model_test_perf = model_performance_regression(dtree_tuned, X_test, y_test)\n","#dtree_tuned_model_test_perf"]},{"cell_type":"markdown","id":"8791198a","metadata":{"id":"8791198a"},"source":["## Hyperparameter Tuning - Bagging Regressor"]},{"cell_type":"code","execution_count":null,"id":"abe1a4c9","metadata":{"id":"abe1a4c9"},"outputs":[],"source":["#Uncomment the below snippet of code if bagging regressor is to be used\n","\n","# # Choose the type of regressor.\n","# bagging_estimator_tuned = BaggingRegressor(random_state=1)\n","# bagging_estimator_tuned = make_pipeline(preprocessor,bagging_estimator_tuned)\n","\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"baggingregressor__max_samples\": ______, #Complete the code to define the list of values to be tuned\n","#     \"baggingregressor__max_features\": _____, #Complete the code to define the list of values to be tuned\n","#     \"baggingregressor__n_estimators\": _____, #Complete the code to define the list of values to be tuned\n","# }\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# bagging_estimator_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# bagging_estimator_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"5cfc3392","metadata":{"id":"5cfc3392"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"70bd7917","metadata":{"id":"70bd7917"},"outputs":[],"source":["#Uncomment the below snippet of code if bagging regressor is to be used\n","\n","\n","# bagging_estimator_tuned_model_train_perf = model_performance_regression(bagging_estimator_tuned, X_train, y_train)\n","# bagging_estimator_tuned_model_train_perf"]},{"cell_type":"markdown","id":"f22573a6","metadata":{"id":"f22573a6"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"76f82ab7","metadata":{"id":"76f82ab7"},"outputs":[],"source":["#Uncomment the below snippet of code if bagging regressor is to be used\n","\n","# bagging_estimator_tuned_model_test_perf = model_performance_regression(bagging_estimator_tuned, X_test, y_test)\n","# bagging_estimator_tuned_model_test_perf"]},{"cell_type":"markdown","id":"6a9e9a8c","metadata":{"id":"6a9e9a8c"},"source":["## Hyperparameter Tuning - Random Forest"]},{"cell_type":"code","execution_count":null,"id":"7ddc78d6","metadata":{"id":"7ddc78d6"},"outputs":[],"source":["#Uncomment the below snippet of code if random forest regressor is to be used\n","\n","\n","# # Choose the type of classifier.\n","# rf_tuned = RandomForestRegressor(random_state=1)\n","# rf_tuned = make_pipeline(preprocessor,rf_tuned)\n","\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"randomforestregressor__max_depth\": ______, #Complete the code to define the list of values to be tuned\n","#     \"randomforestregressor__max_features\":______, #Complete the code to define the list of values to be tuned\n","#     \"randomforestregressor__n_estimators\": ______, #Complete the code to define the list of values to be tuned\n","# }\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(rf_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# rf_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# rf_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"72bad481","metadata":{"id":"72bad481"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"cb83a1a3","metadata":{"id":"cb83a1a3"},"outputs":[],"source":["#Uncomment the below snippet of code if random forest regressor is to be used\n","\n","# rf_tuned_model_train_perf = model_performance_regression(rf_tuned, X_train, y_train)\n","# rf_tuned_model_train_perf"]},{"cell_type":"markdown","id":"634b422d","metadata":{"id":"634b422d"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"b0d6dc2f","metadata":{"id":"b0d6dc2f"},"outputs":[],"source":["#Uncomment the below snippet of code if random forest regressor is to be used\n","\n","# rf_tuned_model_test_perf = model_performance_regression(rf_tuned, X_test, y_test)\n","# rf_tuned_model_test_perf"]},{"cell_type":"markdown","id":"72c3e707","metadata":{"id":"72c3e707"},"source":["## Hyperparameter Tuning - AdaBoost Regressor"]},{"cell_type":"code","execution_count":null,"id":"5876f4cc","metadata":{"id":"5876f4cc"},"outputs":[],"source":["#Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","# # Choose the type of classifier.\n","# ab_tuned = AdaBoostRegressor(random_state=1)\n","# ab_tuned = make_pipeline(preprocessor,ab_tuned)\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"adaboostregressor__n_estimators\": _____, #Complete the code to define the list of values to be tuned\n","#     \"adaboostregressor__learning_rate\": _____, #Complete the code to define the list of values to be tuned\n","# }\n","\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(ab_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# ab_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# ab_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"e12a0341","metadata":{"id":"e12a0341"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"ea98c334","metadata":{"id":"ea98c334"},"outputs":[],"source":["#Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","# ab_tuned_model_train_perf = model_performance_regression(ab_tuned, X_train, y_train)\n","# ab_tuned_model_train_perf"]},{"cell_type":"markdown","id":"571675f8","metadata":{"id":"571675f8"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"e5248f72","metadata":{"id":"e5248f72"},"outputs":[],"source":["#Uncomment the below snippet of code if adaboost regressor is to be used\n","\n","# ab_tuned_model_test_perf = model_performance_regression(ab_tuned, X_test, y_test)\n","# ab_tuned_model_train_perf"]},{"cell_type":"markdown","id":"ed8ca1f0","metadata":{"id":"ed8ca1f0"},"source":["## Hyperparameter Tuning - Gradient Boosting Regressor"]},{"cell_type":"code","execution_count":null,"id":"5da7df63","metadata":{"id":"5da7df63"},"outputs":[],"source":["#Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","\n","# # Choose the type of classifier.\n","# gb_tuned = GradientBoostingRegressor(random_state=1)\n","# gb_tuned = make_pipeline(preprocessor,gb_tuned)\n","\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"gradientboostingregressor__n_estimators\": _____, #Complete the code to define the list of values to be tuned\n","#     \"gradientboostingregressor__subsample\": _____, #Complete the code to define the list of values to be tuned\n","#     \"gradientboostingregressor__max_features\": _____, #Complete the code to define the list of values to be tuned\n","#     \"gradientboostingregressor__max_depth\": _____, #Complete the code to define the list of values to be tuned\n","# }\n","\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(gb_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# gb_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# gb_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"5c4d7912","metadata":{"id":"5c4d7912"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"2f75d710","metadata":{"id":"2f75d710"},"outputs":[],"source":["#Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","# gb_tuned_model_train_perf = model_performance_regression(gb_tuned, X_train, y_train)\n","# gb_tuned_model_train_perf"]},{"cell_type":"markdown","id":"cb89629a","metadata":{"id":"cb89629a"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"fa9a0bd4","metadata":{"id":"fa9a0bd4"},"outputs":[],"source":["#Uncomment the below snippet of code if gradientboost regressor is to be used\n","\n","# gb_tuned_model_test_perf = model_performance_regression(gb_tuned, X_test, y_test)\n","# gb_tuned_model_test_perf"]},{"cell_type":"markdown","id":"aced13be","metadata":{"id":"aced13be"},"source":["## Hyperparameter Tuning - XGBoost Regressor"]},{"cell_type":"code","execution_count":null,"id":"7a018bda","metadata":{"id":"7a018bda"},"outputs":[],"source":["#Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","\n","# # Choose the type of classifier.\n","# xgb_tuned = XGBRegressor(random_state=1)\n","# xgb_tuned = make_pipeline(preprocessor,xgb_tuned)\n","\n","# # Grid of parameters to choose from\n","# parameters = {\n","#     \"xgbregressor__n_estimators\": _____, #Complete the code to define the list of values to be tuned\n","#     \"xgbregressor__subsample\": _____, #Complete the code to define the list of values to be tuned\n","#     \"xgbregressor__gamma\": _____, #Complete the code to define the list of values to be tuned\n","#     \"xgbregressor__colsample_bytree\": _____, #Complete the code to define the list of values to be tuned\n","#     \"xgbregressor__colsample_bylevel\": _____, #Complete the code to define the list of values to be tuned\n","# }\n","\n","# # Run the grid search\n","# grid_obj = GridSearchCV(xgb_tuned, parameters, scoring=r2_score, cv=3, n_jobs = -1)\n","# grid_obj = grid_obj.fit(X_train, y_train)\n","\n","# # Set the clf to the best combination of parameters\n","# xgb_tuned = grid_obj.best_estimator_\n","\n","# # Fit the best algorithm to the data.\n","# xgb_tuned.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"14e10ca1","metadata":{"id":"14e10ca1"},"source":["### Checking model performance on training set"]},{"cell_type":"code","execution_count":null,"id":"cba48890","metadata":{"id":"cba48890"},"outputs":[],"source":["#Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","# xgb_tuned_model_train_perf = model_performance_regression(xgb_tuned, X_train, y_train)\n","# xgb_tuned_model_train_perf"]},{"cell_type":"markdown","id":"377fe3e2","metadata":{"id":"377fe3e2"},"source":["### Checking model performance on test set"]},{"cell_type":"code","execution_count":null,"id":"a4e80152","metadata":{"id":"a4e80152"},"outputs":[],"source":["#Uncomment the below snippet of code if xgboost regressor is to be used\n","\n","# xgb_tuned_model_test_perf = model_performance_regression(xgb_tuned, X_test, y_test)\n","# xgb_tuned_model_test_perf"]},{"cell_type":"markdown","id":"b0810287","metadata":{"id":"b0810287"},"source":["# **Model Performance Comparison, Final Model Selection, and Serialization**"]},{"cell_type":"code","execution_count":null,"id":"a06d9858","metadata":{"id":"a06d9858"},"outputs":[],"source":[" # training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the train performance metrics of the first model you have choosen . Eg, rf_model_train_perf\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the train performance metrics of the first model (tuned) you have choosen\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the train performance metrics of the second model you have choosen\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the train performance metrics of the second model (tuned) you have choosen\n","    ],\n","    axis=1,\n",")\n","\n","models_train_comp_df.columns = [\"_____\"] #Complete the code to define the names for the models\n","\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","execution_count":null,"id":"868c9619","metadata":{"id":"868c9619","scrolled":true},"outputs":[],"source":[" # training performance comparison\n","\n","models_train_comp_df = pd.concat(\n","    [\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the test performance metrics of the first model you have choosen . Eg, rf_model_test_perf\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the test performance metrics of the first model (tuned) you have choosen\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the test performance metrics of the second model you have choosen\n","        _____.T, #Complete the code to define the variable name of the dataframe which stores the test performance metrics of the second model (tuned) you have choosen\n","    ],\n","    axis=1,\n",")\n","\n","models_train_comp_df.columns = [\"_____\"] #Complete the code to define the names for the models\n","\n","print(\"Training performance comparison:\")\n","models_train_comp_df"]},{"cell_type":"code","execution_count":null,"id":"SwIewqGpiRtW","metadata":{"id":"SwIewqGpiRtW"},"outputs":[],"source":["# Create a folder for storing the files needed for web app deployment\n","os.makedirs(\"backend_files\", exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"E5-0ybugEl_5","metadata":{"id":"E5-0ybugEl_5"},"outputs":[],"source":["# Define the file path to save (serialize) the trained model along with the data preprocessing steps\n","saved_model_path = \"backend_files/_____.joblib\" #Complete the code to define the name of the model"]},{"cell_type":"code","execution_count":null,"id":"mvvTI1WnFE3C","metadata":{"id":"mvvTI1WnFE3C"},"outputs":[],"source":["# Save the best trained model pipeline using joblib\n","joblib.dump(_____, saved_model_path) #Complete the code to pass the variable name of the best model\n","\n","print(f\"Model saved successfully at {saved_model_path}\")"]},{"cell_type":"code","execution_count":null,"id":"uNR7Dxj2MTM9","metadata":{"id":"uNR7Dxj2MTM9"},"outputs":[],"source":["# Load the saved model pipeline from the file\n","saved_model = joblib.load(\"backend_files/_____.joblib\") #Complete the code to define the name of the saved model\n","\n","# Confirm the model is loaded\n","print(\"Model loaded successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"LoPE6PZUM7Uh","metadata":{"id":"LoPE6PZUM7Uh"},"outputs":[],"source":["saved_model"]},{"cell_type":"markdown","id":"UJMu1yQztach","metadata":{"id":"UJMu1yQztach"},"source":["Let's try making predictions on the test set using the deserialized model.\n","\n","- Please ensure that the saved model is loaded before making predictions."]},{"cell_type":"code","execution_count":null,"id":"MzpxBjtSM9f-","metadata":{"id":"MzpxBjtSM9f-"},"outputs":[],"source":["saved_model.predict(_____) #Complete the code to pass the X_test for inference"]},{"cell_type":"markdown","id":"oxj1WOs1uxt_","metadata":{"id":"oxj1WOs1uxt_"},"source":["- As we can see, the model can be directly used for making predictions without any retraining."]},{"cell_type":"markdown","id":"9a2LCguV-7i1","metadata":{"id":"9a2LCguV-7i1"},"source":["# **Deployment - Backend**"]},{"cell_type":"markdown","id":"T3XlDPUtJnDo","metadata":{"id":"T3XlDPUtJnDo"},"source":["## Flask Web Framework\n"]},{"cell_type":"code","execution_count":null,"id":"pwUlhcXQFJs4","metadata":{"id":"pwUlhcXQFJs4"},"outputs":[],"source":["%%writefile backend_files/app.py\n","\n","# Import necessary libraries\n","import numpy as np\n","import joblib  # For loading the serialized model\n","import pandas as pd  # For data manipulation\n","from flask import Flask, request, jsonify  # For creating the Flask API\n","\n","# Initialize Flask app with a name\n","superkart_api = Flask(\"_____\") #Complete the code to define the name of the app\n","\n","# Load the trained churn prediction model\n","model = joblib.load(\"_____\") #Complete the code to define the location of the serialized model\n","\n","# Define a route for the home page\n","@superkart_api.get('/')\n","def home():\n","    return \"_____\" #Complete the code to define a welcome message\n","\n","# Define an endpoint to predict churn for a single customer\n","@superkart_api.post('/v1/predict')\n","def predict_sales():\n","    # Get JSON data from the request\n","    data = request.get_json()\n","\n","    # Extract relevant customer features from the input data. The order of the column names matters.\n","    sample = {\n","        'Product_Weight': data['Product_Weight'],\n","        'Product_Sugar_Content': data['Product_Sugar_Content'],\n","        'Product_Allocated_Area': data['Product_Allocated_Area'],\n","        'Product_MRP': data['Product_MRP'],\n","        'Store_Size': data['Store_Size'],\n","        'Store_Location_City_Type': data['Store_Location_City_Type'],\n","        'Store_Type': data['Store_Type'],\n","        'Product_Id_char': data['Product_Id_char'],\n","        'Store_Age_Years': data['Store_Age_Years'],\n","        'Product_Type_Category': data['Product_Type_Category']\n","    }\n","\n","    # Convert the extracted data into a DataFrame\n","    input_data = pd.DataFrame([sample])\n","\n","    # Make a churn prediction using the trained model\n","    prediction = model.predict(input_data).tolist()[0]\n","\n","    # Return the prediction as a JSON response\n","    return jsonify({'Sales': prediction})\n","\n","\n","# Run the Flask app in debug mode\n","if __name__ == '__main__':\n","    superkart_api.run(debug=True)"]},{"cell_type":"markdown","id":"STDSb04iT-rL","metadata":{"id":"STDSb04iT-rL"},"source":["## Dependencies File"]},{"cell_type":"code","execution_count":null,"id":"vKj1zTp-FJm4","metadata":{"id":"vKj1zTp-FJm4"},"outputs":[],"source":["%%writefile backend_files/requirements.txt\n","pandas==2.2.2\n","numpy==2.0.2\n","scikit-learn==1.6.1\n","seaborn==0.13.2\n","joblib==1.4.2\n","xgboost==2.1.4\n","joblib==1.4.2\n","Werkzeug==2.2.2\n","flask==2.2.2\n","gunicorn==20.1.0\n","requests==2.32.3\n","uvicorn[standard]\n","streamlit==1.43.2"]},{"cell_type":"markdown","id":"JWD7rPCRUEtD","metadata":{"id":"JWD7rPCRUEtD"},"source":["## Dockerfile"]},{"cell_type":"code","execution_count":null,"id":"fujFE1-fFJkY","metadata":{"id":"fujFE1-fFJkY"},"outputs":[],"source":["%%writefile backend_files/Dockerfile\n","FROM python:3.9-slim\n","\n","# Set the working directory inside the container\n","_____ /app #Complete the code to mention the command in Docker to set the working directory\n","\n","# Copy all files from the current directory to the container's working directory\n","_____ . . #Complete the code to mention the command in Docker to copy the files from the current directory to the container's working directory\n","\n","# Install dependencies from the requirements file without using cache to reduce image size\n","_____ pip install --no-cache-dir --upgrade -r requirements.txt #Complete the code to mention the command in Docker to install dependencies\n","\n","# Define the command to start the application using Gunicorn with 4 worker processes\n","# - `-w 4`: Uses 4 worker processes for handling requests\n","# - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces\n","# - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)\n","CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:superkart_api\"]"]},{"cell_type":"markdown","id":"yK1n7jBcRrYr","metadata":{"id":"yK1n7jBcRrYr"},"source":["## Setting up a Hugging Face Docker Space for the Backend"]},{"cell_type":"markdown","id":"xh47A77YaIEt","metadata":{"id":"xh47A77YaIEt"},"source":["**Note**: We are creating a Hugging Face Docker Space for our backend using the Hugging Face Hub API. This automates the space creation process and enables seamless deployment of our Flask app."]},{"cell_type":"code","execution_count":null,"id":"VvfrEZl0Xe7C","metadata":{"id":"VvfrEZl0Xe7C"},"outputs":[],"source":["# Import the login function from the huggingface_hub library\n","from huggingface_hub import login\n","\n","# Login to your Hugging Face account using your access token\n","# Replace \"YOUR_HUGGINGFACE_TOKEN\" with your actual token\n","#login(token=\"YOUR_HUGGINGFACE_TOKEN\")  # You can get your token from https://huggingface.co/settings/tokens\n","login(token=\"_____\") #Complete the code to define the access token\n","\n","# Import the create_repo function from the huggingface_hub library\n","from huggingface_hub import create_repo"]},{"cell_type":"code","execution_count":null,"id":"MvE0kOmBSAsw","metadata":{"id":"MvE0kOmBSAsw"},"outputs":[],"source":["# Try to create the repository for the Hugging Face Space\n","try:\n","    create_repo(\"_____\",  #Complete the code to define the name of the repository\n","        repo_type=\"space\",  # Specify the repository type as \"space\"\n","        space_sdk=\"docker\",  # Specify the space SDK as \"docker\"\n","        private=False  # Set to True if you want the space to be private\n","    )\n","except Exception as e:\n","    # Handle potential errors during repository creation\n","    if \"RepositoryAlreadyExistsError\" in str(e):\n","        print(\"Repository already exists. Skipping creation.\")\n","    else:\n","        print(f\"Error creating repository: {e}\")"]},{"cell_type":"markdown","id":"B4tnVrlo8xQ9","metadata":{"id":"B4tnVrlo8xQ9"},"source":["## Uploading Files to Hugging Face Space (Docker Space)"]},{"cell_type":"markdown","id":"0thJ9hWq_aqU","metadata":{"id":"0thJ9hWq_aqU"},"source":["**Note**: Before running the code below, ensure that the serialized ML model has been uploaded in to `backend_files` folder."]},{"cell_type":"code","execution_count":null,"id":"JqcGeow684xg","metadata":{"id":"JqcGeow684xg"},"outputs":[],"source":["# for hugging face space authentication to upload files\n","\n","access_key = \"_____\"  #Complete the code to define the access token\n","repo_id = \"_____\"  #Complete the code to define the repo id.\n","\n","# Login to Hugging Face platform with the access token\n","login(token=access_key)\n","\n","# Initialize the API\n","api = HfApi()\n","\n","# Upload Streamlit app files stored in the folder called deployment_files\n","api.upload_folder(\n","    folder_path=\"backend_files\",\n","    repo_id=repo_id,  # Hugging face space id\n","    repo_type=\"space\",  # Hugging face repo type \"space\"\n",")"]},{"cell_type":"markdown","id":"bv07DWg0_G6L","metadata":{"id":"bv07DWg0_G6L"},"source":["# **Deployment - Frontend**"]},{"cell_type":"markdown","source":["## Points to note before executing the below cells\n","\n","- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1"],"metadata":{"id":"MklmsOTFTq3r"},"id":"MklmsOTFTq3r"},{"cell_type":"markdown","id":"UsCYxkq_UL3Q","metadata":{"id":"UsCYxkq_UL3Q"},"source":["## Streamlit for Interactive UI"]},{"cell_type":"code","execution_count":null,"id":"eOcW-xasOHZm","metadata":{"id":"eOcW-xasOHZm"},"outputs":[],"source":["# Create a folder for storing the files needed for frontend UI deployment\n","os.makedirs(\"frontend_files\", exist_ok=True)"]},{"cell_type":"code","execution_count":null,"id":"McWDBwZPFJe6","metadata":{"id":"McWDBwZPFJe6"},"outputs":[],"source":["%%writefile frontend_files/app.py\n","\n","import streamlit as st\n","import requests\n","\n","st.title(\"_____\") #Complete the code to define the title of the app.\n","\n","# Input fields for product and store data\n","Product_Weight = st.number_input(\"Product Weight\", min_value=0.0, value=12.66)\n","Product_Sugar_Content = st.selectbox(\"Product Sugar Content\", [\"Low Sugar\", \"Regular\", \"No Sugar\"])\n","Product_Allocated_Area = ______ #Complete the code to define the UI element for Product_Allocated_Area\n","Product_MRP = ______ #Complete the code to define the UI element for Product_MRP\n","Store_Size = _____ ______ #Complete the code to define the UI element for Store_Size\n","Store_Location_City_Type = _____ ______ #Complete the code to define the UI element for Store_Location_City_Type\n","Store_Type = _____ ______ #Complete the code to define the UI element for Store_Type\n","Product_Id_char = _____ ______ #Complete the code to define the UI element for Product_Id_char\n","Store_Age_Years = _____ ______ #Complete the code to define the UI element for Store_Age_Years\n","Product_Type_Category = _____ ______ #Complete the code to define the UI element for Product_Type_Category\n","\n","product_data = {\n","    \"Product_Weight\": Product_Weight,\n","    \"Product_Sugar_Content\": Product_Sugar_Content,\n","    \"Product_Allocated_Area\": Product_Allocated_Area,\n","    \"Product_MRP\": Product_MRP,\n","    \"Store_Size\": Store_Size,\n","    \"Store_Location_City_Type\": Store_Location_City_Type,\n","    \"Store_Type\": Store_Type,\n","    \"Product_Id_char\": Product_Id_char,\n","    \"Store_Age_Years\": Store_Age_Years,\n","    \"Product_Type_Category\": Product_Type_Category\n","}\n","\n","if st.button(\"Predict\", type='primary'):\n","    response = requests.post(\"https://<user_name>-<space_name>.hf.space/v1/predict\", json=product_data)    # Complete the code to enter user name and space name to correctly define the endpoint\n","    if response.status_code == 200:\n","        result = response.json()\n","        predicted_sales = result[\"Sales\"]\n","        st.write(f\"Predicted Product Store Sales Total: ₹{predicted_sales:.2f}\")\n","    else:\n","        st.error(\"Error in API request\")"]},{"cell_type":"markdown","id":"beq1RbMhUQmi","metadata":{"id":"beq1RbMhUQmi"},"source":["## Dependencies File"]},{"cell_type":"code","execution_count":null,"id":"_mRb5eXRR7u-","metadata":{"id":"_mRb5eXRR7u-"},"outputs":[],"source":["%%writefile frontend_files/requirements.txt\n","requests==2.32.3\n","streamlit==1.45.0"]},{"cell_type":"markdown","source":["## Dockerfile"],"metadata":{"id":"wxMWl4bQwTAP"},"id":"wxMWl4bQwTAP"},{"cell_type":"code","source":["%%writefile frontend_files/Dockerfile\n","# Use a minimal base image with Python 3.9 installed\n","FROM python:3.9-slim\n","\n","# Set the working directory inside the container to /app\n","WORKDIR /app\n","\n","# Copy all files from the current directory on the host to the container's /app directory\n","COPY . .\n","\n","# Install Python dependencies listed in requirements.txt\n","RUN pip3 install -r requirements.txt\n","\n","# Define the command to run the Streamlit app on port 8501 and make it accessible externally\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]\n","\n","# NOTE: Disable XSRF protection for easier external access in order to make batch predictions"],"metadata":{"id":"7uFBgZd_wUa3"},"id":"7uFBgZd_wUa3","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"5Re8ovwv9Rb5","metadata":{"id":"5Re8ovwv9Rb5"},"source":["## Uploading Files to Hugging Face Space (Streamlit Space)"]},{"cell_type":"code","execution_count":null,"id":"4b93nx2F9Qt0","metadata":{"id":"4b93nx2F9Qt0"},"outputs":[],"source":["access_key = \"_____\"  #Complete the code to define the access token\n","repo_id = \"_____\"  #Complete the code to define the repo id\n","\n","# Login to Hugging Face platform with the access token\n","login(token=access_key)\n","\n","# Initialize the API\n","api = HfApi()\n","\n","# Upload Streamlit app files stored in the folder called deployment_files\n","api.upload_folder(\n","    folder_path=\"frontend_files\",\n","    repo_id=repo_id,  # Hugging face space id\n","    repo_type=\"space\",  # Hugging face repo type \"space\"\n",")"]},{"cell_type":"markdown","id":"e4213339","metadata":{"id":"e4213339"},"source":["# **Actionable Insights and Business Recommendations**"]},{"cell_type":"markdown","source":["-\n","\n","-"],"metadata":{"id":"gzTwDHFNZMjo"},"id":"gzTwDHFNZMjo"}],"metadata":{"colab":{"collapsed_sections":["k-xVqOB6QTm6","AJDbTeirQasg","Aasy7LC_Qpq5","v-HxlIhTQ0-E","EWGlpDNkrTqA","60VhOlydQ-PG","51b91836","W2sXwrUERYua","65c8c14d","ee04adcd","e25abfe4","9e76c12a","J3auOiwfUXWf","w0SIhAByUoHh","4oamAwxrVHLq","21341d2b","afe06238","a23bdd73","a3242fb5","2a53e9ac","af2e3e97","65a65843","da06e6d9","6a20880d","971f7446","9475b863","46f0b9af","05mWTy8wMUKG","02b68b47","9292a578","7301694d","e8ecab60","02523ef7","9d6b86b0","c2bd287f","88533e8f","f47c5e18","d1bb6c77","09d79669","4e457801","937e0981","ae5b4f72","b5ecd021","0fo5OvIfVdtB","12651542","2bf4362a","Ey6xQv8Eal1x","27b7992e","fef26ce9","-8pT_8miXe8I","ORwoDUPvkqEt","5fd3cabe","YyzOQ8pBY93N","c498d89d","f5be37da","118e3496","e4980b89","206f5d57","ffbda60a","FtkIDTjdYy5h","37341729","8791198a","6a9e9a8c","72c3e707","ed8ca1f0","aced13be","b0810287","9a2LCguV-7i1","T3XlDPUtJnDo","STDSb04iT-rL","JWD7rPCRUEtD","yK1n7jBcRrYr","B4tnVrlo8xQ9","bv07DWg0_G6L","UsCYxkq_UL3Q","e4213339"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}